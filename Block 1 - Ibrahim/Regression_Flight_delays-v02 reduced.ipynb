{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eHZUg2aZugH"
   },
   "source": [
    "# Import and read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ibrago/anaconda3/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in /home/ibrago/anaconda3/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /home/ibrago/anaconda3/lib/python3.7/site-packages (from xgboost) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "! pip  install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P_PizJ6WhIw6"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries and load in the dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/Jan_2019_usdot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLQoxdqMbnJT"
   },
   "outputs": [],
   "source": [
    "#read in data 2019\n",
    "df_jan2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/Jan_2019_usdot.csv')\n",
    "#df_feb2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/Feb_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')\n",
    "#df_2019 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2019_usdot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUfnCOiqbpWM"
   },
   "outputs": [],
   "source": [
    "#read in data 2020\n",
    "\n",
    "#df_jan2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/Jan_2020_usdot.csv')\n",
    "#df_feb2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/Feb_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')\n",
    "#df_2020 = pd.read_csv('https://niy-certification.s3.eu-central-1.amazonaws.com/usdot/_2020_usdot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ7ZTRFWvui2"
   },
   "outputs": [],
   "source": [
    "df_jan2019_cp = df_jan2019.copy()\n",
    "df_feb2019_cp = df_feb2019.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEq6EM6Raju_"
   },
   "source": [
    "# Describe Dataset and some data cleaning (no EDA - done in different Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIOH60e3hpMz",
    "outputId": "f4a7badb-1925-416b-bf92-d11bb98de641"
   },
   "outputs": [],
   "source": [
    "jan19 = df_jan2019_cp.columns\n",
    "feb19 = df_feb2019_cp.columns\n",
    "print('existing different cols in jan19 and feb 19 cols:', list(set(jan19) - set(feb19)))\n",
    "# will have a brief look to both data. If no difference simple data preprocessing will be executed on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vcS8DArPi3Kx",
    "outputId": "fd357136-3aa3-4892-c0da-3024e96519cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show some more information for each column\n",
    "def my_info(df):\n",
    "  info = pd.DataFrame(df.count(), columns=[\"filled\"])\n",
    "  info[\"filled %\"] = round(info[\"filled\"] / len(df), 4) * 100\n",
    "  info[\"nunique\"] = df.nunique()\n",
    "  info[\"dtypes\"] = df.dtypes\n",
    "  info[\"uniques\"] = np.nan\n",
    "  for idx, row in info.iterrows():\n",
    "    info.loc[idx, \"uniques\"] = str(list(df[idx].unique()))\n",
    "  return info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_info(df_jan2019_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNPdemflyUCK"
   },
   "source": [
    "for the first test \n",
    "- drop all dtype = object\n",
    "- delete cols with less than 20% value count\n",
    "- delte arrival and departure related columns / leave wheels_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTDbnSHfaxI2",
    "outputId": "91abbc1b-f590-4b58-e44e-b8c2f6a990fd"
   },
   "outputs": [],
   "source": [
    "# finding out some infos on columns\n",
    "#show some more information for each column\n",
    "\n",
    "dflist = [df_jan2019_cp, df_feb2019_cp]\n",
    "for item in dflist:\n",
    "  print(item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6duuJzNgi9TE",
    "outputId": "456842e1-fb4f-4edd-e6a4-4da2f1c6db35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping columns les than 20% filled columns\n",
    "delay_related_cols = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'Unnamed: 54', \\\n",
    "                      'ARR_TIME', 'ARR_DELAY', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'AIR_TIME', \\\n",
    "                      'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'WHEELS_ON', 'DEP_TIME_BLK', \\\n",
    "                      'ACTUAL_ELAPSED_TIME',]\n",
    "\n",
    "for item in dflist:\n",
    "  for col in delay_related_cols:\n",
    "    item.drop(col, axis=1, inplace = True)\n",
    "  print(item.shape)\n",
    "\n",
    "for item in dflist:\n",
    "  item.dropna(inplace= True)\n",
    "  print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_relevant_object_cols = ['FL_DATE', 'OP_CARRIER', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_NM', 'DEST_CITY_NAME', \\\n",
    "                            'DEST_STATE_ABR', 'DEST_STATE_NM', 'ARR_TIME_BLK', 'TAIL_NUM']\n",
    "df_jan2019_cp.drop(not_relevant_object_cols, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan2019_cp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "df_jan2019_cp = df_jan2019_cp.sample(10000)\n",
    "enc.fit(df_jan2019_cp)\n",
    "\n",
    "enc.categories_\n",
    "\n",
    "enc.fit(df_jan2019_cp.select_dtypes(include=['object']))\n",
    "enc.categories_\n",
    "\n",
    "enc.get_feature_names()\n",
    "\n",
    "ohe_df_jan2019 = enc.transform(df_jan2019_cp.select_dtypes(include=['object'])).toarray()\n",
    "\n",
    "# droppen der kategorialen spalten aus dem Datensatz\n",
    "# hinzufügen der transformierten daten + spaltennamen\n",
    "\n",
    "\n",
    "df_temp = pd.DataFrame(columns=enc.get_feature_names(), index=df_jan2019_cp.index, data=ohe_df_jan2019)\n",
    "\n",
    "df_jan2019_cp_ohe = pd.concat([df_jan2019_cp.drop(df_jan2019_cp.select_dtypes(include=['object']).columns, axis=1), df_temp], axis=1)\n",
    "df_jan2019_cp_ohe\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_info(df_jan2019_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQsU-hYlpNmE"
   },
   "source": [
    "# Regression - XGBoost including 'wheels_off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqLrTp6F1bj_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "def xg_proc(df):\n",
    "\n",
    "    # Define features and label\n",
    "  X = df.drop('ARR_DELAY_NEW', axis=1)\n",
    "  y = df['ARR_DELAY_NEW']\n",
    "  \n",
    "  # Split into train and test splits using the random state of 42 and a test size of 20%\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y, test_size=0.2, random_state=42)\n",
    "  \n",
    "  # One hot Encode\n",
    "  enc.fit(X_train.select_dtypes(include=['object']))\n",
    "  X_train_ohe_array = enc.transform(X_train.select_dtypes(include=['object'])).toarray()\n",
    "  X_test_ohe_array = enc.transform(X_test.select_dtypes(include=['object'])).toarray()\n",
    "\n",
    "  # droppen der kategorialen spalten aus dem Datensatz\n",
    "  # hinzufügen der transformierten daten + spaltennamen\n",
    "  df_temp = pd.DataFrame(columns=enc.get_feature_names(), index=X_train.index, data=X_train_ohe_array)\n",
    "  X_train = pd.concat([X_train.drop(X_train.select_dtypes(include=['object']).columns, axis=1), df_temp], axis=1)\n",
    "\n",
    "  df_temp = pd.DataFrame(columns=enc.get_feature_names(), index=X_test.index, data=X_test_ohe_array)\n",
    "  X_test = pd.concat([X_test.drop(X_test.select_dtypes(include=['object']).columns, axis=1), df_temp], axis=1)\n",
    "\n",
    "  # Create the XGBoost Regressor\n",
    "  xg_reg = xgb.XGBRegressor()\n",
    "    \n",
    "  # Fit the Regressor\n",
    "  xg_reg.fit(X_train, y_train)\n",
    "    \n",
    "  # Predict the response for the test dataset\n",
    "  y_pred = xg_reg.predict(X_test)\n",
    "  # Print r2_score\n",
    "  r2_xgb = r2_score(y_test, y_pred)\n",
    "    \n",
    "  return r2_xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "id": "5XP5Zy8rmnP6",
    "outputId": "439d8cff-f4d8-4cd7-bcd0-f49e016804f0"
   },
   "outputs": [],
   "source": [
    "d = {'Model and feature': ['XGB with WHEELS_OFF'], 'r2_score model': [xg_proc(df_jan2019_cp)], 'r2score_w_Hyperparameter': [0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "param_tuning = {\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'subsample': [0.5, 0.7],\n",
    "            'colsample_bytree': [0.5, 0.7],\n",
    "            'n_estimators' : [100, 200, 500],\n",
    "            'objective': ['reg:squarederror']\n",
    "        }\n",
    "\n",
    "\n",
    "def hyperparameter_tuning(df, param_tuning):\n",
    "\n",
    "    # Define features and label\n",
    "    X = df.drop('ARR_DELAY_NEW', axis=1)\n",
    "    y = df['ARR_DELAY_NEW']\n",
    "\n",
    "    # Split into train and test splits using the random state of 42 and a test size of 20%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # One hot Encode\n",
    "    enc.fit(X_train.select_dtypes(include=['object']))\n",
    "    X_train_ohe_array = enc.transform(X_train.select_dtypes(include=['object'])).toarray()\n",
    "    X_test_ohe_array = enc.transform(X_test.select_dtypes(include=['object'])).toarray()\n",
    "\n",
    "    # droppen der kategorialen spalten aus dem Datensatz\n",
    "    # hinzufügen der transformierten daten + spaltennamen\n",
    "    df_temp = pd.DataFrame(columns=enc.get_feature_names(), index=X_train.index, data=X_train_ohe_array)\n",
    "    X_train = pd.concat([X_train.drop(X_train.select_dtypes(include=['object']).columns, axis=1), df_temp], axis=1)\n",
    "\n",
    "    df_temp = pd.DataFrame(columns=enc.get_feature_names(), index=X_test.index, data=X_test_ohe_array)\n",
    "    X_test = pd.concat([X_test.drop(X_test.select_dtypes(include=['object']).columns, axis=1), df_temp], axis=1)\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor()\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = xgb_model,\n",
    "                           param_grid = param_tuning,                        \n",
    "                           cv = 5,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1)\n",
    "\n",
    "    gsearch.fit(X_train,y_train)\n",
    "\n",
    "    return gsearch.best_params_\n",
    "\n",
    "best_params = hyperparameter_tuning(df_jan2019_cp, param_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking out 'wheels_off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan2019_cp_wo = df_jan2019_cp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheels_off_col = ['WHEELS_OFF']\n",
    "\n",
    "for item in dflist:\n",
    "  for col in wheels_off_col:\n",
    "    item.drop(col, axis=1, inplace = True)\n",
    "  print(item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "BhZvLrN7p1aL",
    "outputId": "82ea9401-2dde-4731-8716-7363644adb7a"
   },
   "outputs": [],
   "source": [
    "d = {'Model and feature': ['take out wheels_off'], 'r2_score model': [xg_proc(df_jan2019_cp_wo)], 'r2score_w_Hyperparameter': [0]}\n",
    "df2 = pd.DataFrame(data=d)\n",
    "df = df.append(df2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvo3jymvpizM"
   },
   "source": [
    "# Regression Comparison\n",
    "Now, using matplotlib, plot the $R^2$ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvhohRYbpgPX"
   },
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n",
    "df['r2_score model'].astype(float)\n",
    "df.plot(x= 'Model and feature', y = 'r2_score model', kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOujHqxdp4o6"
   },
   "source": [
    "# Main Takeaways\n",
    "\n",
    "\n",
    "1.   Took new Dataset from https://www.transtats.bts.gov/Databases.asp?Mode_ID=1&Mode_DESC=Aviation&Subject_ID2=3\n",
    "2.   Made an XGboost on whole dataset with some basic cleaning -> Strong Score as other columns with delay information are avaiable\n",
    "3.   After taking out arrivel delay related columns, no strong implication as departure delay show high dependancy\n",
    "4.   Taking out departure delay information leeds to massive decline of r2 score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression Flight delays.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
